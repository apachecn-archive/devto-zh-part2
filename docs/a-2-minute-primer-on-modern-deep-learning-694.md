# 现代深度学习的 2 分钟初级读本

> 原文：<https://dev.to/harshsikka123/a-2-minute-primer-on-modern-deep-learning-694>

在大多数情况下，深度学习(DL)实际上只是一种统计方法的应用，使用具有几层的神经网络架构来找出数据中的模式。

神经网络通常遵循一般的设计。有一些输入表示正在馈送到网络的数据，即图像中的像素，后面是几层隐藏节点，这些节点在传递信息之前以特定的方式处理信息。

深度学习中的“深”其实只是对这些存在于你的输入层和输出层之间的隐藏层的引用。正如你可能已经从名字中推断出的那样，这种架构非常松散地受到了大脑的启发。松散地强调。

[![sample neural network](../Images/a513805de7d253cb63b5b828cb45cc4c.png "Credits: Stanford CS229")T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--5p3_EcTZ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/i2su6hiu9ri1r7g9lz7n.jpeg)

为了训练这些网络，我们向它们展示了大量带有标签的样本数据示例。(想象一下给它看一张标签为“狗”的德国牧羊犬的照片)我们通过一种称为反向传播的算法，采用一种称为梯度下降的过程来修改节点之间的连接。这使我们能够修剪我们的网络并加强输入-输出映射。当我们训练它们时，基于 DL 的网络经常形成对应于输入的一些子成分的数据的有趣表示。想象一下代表桌子边缘的线条。

有许多有趣的子架构可以提供优化的或递增的更好的结果，尤其是在特定的问题领域，但是我们不会在这里深入探讨。

如果你只能从这本初级读本中学到一件事，那就让它成为:

在拥有无限数据和计算能力的宇宙中，DL 技术可以对输入和输出之间的任何离散表示进行编码，这是一个非常强大的概念。例如，有大量的问题涉及到对某些输入进行分类。当然，类别可以代表几乎任何东西，所以给 DL 足够的数据和计算，它可能映射到你给它的任何输入输出对。

实际上，有许多地方性的问题，包括陷入局部最小值的概念，尽管有更好的解决方案，但你的算法认为它是优化的。这通常是可以解决的，在处理足够大量的数据时，这通常不是一个大问题。

TLDR: DL 拥有强大的数据量和计算能力，可以有效地解决简单的映射问题。但是如果目标是对因果关系和智力进行编码，它会怎么做呢？我很快会检查的。

*这是介绍具有* *自适应拓扑的模块化神经网络的几个部分系列的第 1 部分，我认为这种架构对于解决* *复杂的分层问题会非常成功。*

你可以在这里或在 [technomancy 上查看该系列的其余部分。](https://medium.com/technomancy)T3】