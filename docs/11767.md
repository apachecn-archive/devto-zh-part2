# AlphaGo:关于机器智能的观察

> 原文:[https://dev . to/nested software/alpha go-observations-about-machine-intelligence-4c 62](https://dev.to/nestedsoftware/alphago-observations-about-machine-intelligence-4c62)

## DeepMind 和 AlphaGo

我喜欢玩 [go](https://en.wikipedia.org/wiki/Go_(game)) 的游戏(不要和编程语言混淆)。它在韩国也被称为八卦掌，在中国被称为围棋。在过去的几年里， [DeepMind](https://deepmind.com/) 与 [AlphaGo](https://deepmind.com/research/alphago/) 在围棋和人工智能领域进行了一场深刻的革命。在 AlphaGo 之前，基于 MCTS 或[蒙特卡罗树搜索](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)的最佳围棋人工智能相对较弱。一个强有力的业余选手能打败他们，他们不可能战胜职业选手。

DeepMind 的 AlphaGo 使用[深度学习](https://en.wikipedia.org/wiki/Deep_learning)，改变了这一切。2016 年 1 月，DeepMind 发布了 AlphaGo 在一系列连赢比赛中击败中国退役职业棋手范辉的消息。当时，go 社区认为这样一个里程碑还需要大约 10 年的时间。

后来，在 2016 年春天，AlphaGo 在一场 5 局系列赛中以 4-1 击败了世界上最好的现役棋手之一李·塞多尔(Lee Sedol)。2017 年 5 月，一个更强大的 AlphaGo 版本在 3 场比赛中以 3 比 0 击败了世界排名第一的选手柯洁。几个月前，一个类似的版本以“大师”的名义在快速控制时间的在线游戏中以 60 比 0 击败了所有世界顶级职业选手。

## AlphaGo Zero

不过，最有趣的发展可能是在那之后。这些早期版本的 AlphaGo 使用神经网络来学习如何下围棋，但他们关于什么代表好棋的想法受到了用于引导网络训练的人类游戏的影响。

AlphaGo Zero 在 2017 年秋季的一篇自然论文中描述，它在不使用任何人类游戏的情况下，通过与自己对弈，学会了如何完全靠自己下围棋。它以随机移动开始，仅经过 3 天的训练就迅速成为超人(ELO 约为 4500)。之后，DeepMind 再次从头开始训练它，这次用了 40 天，产生了一个 ELO 估计超过 5000 的 AI。相比之下，顶级人类玩家的 ELO 大约为 [3600](https://www.goratings.org/en/) 。

此外，AlphaGo 的早期版本确实在人工智能中编码了几个启发式算法。例如，在围棋中有一个概念叫做[阶梯](https://en.wikipedia.org/wiki/Ladder_(Go))。在早期版本中，读出梯子是手工编码的。所有这样的启发式算法在 Zero 版本中都被删除了，所以除了规则之外，它必须完全靠自己来学习所有关于 go 的东西。

> 梯子是一种情况，你可以沿着对角线追一群人。如果棋盘另一边的正确位置有一个友好的棋子(称为石头),则该组不能被捕获。如果没有，那就可以。这是初学者在学习围棋规则后几乎马上就会学到的东西，但事实证明人工智能自己学习这个概念并不容易。

AlphaGo Zero 经过 40 天的训练，超越了 AlphaGo 之前所有版本的能力。它现在被广泛认为是世界上最强的围棋人工智能，明显强于任何人类棋手。

这是人工智能的巨大成就。虽然蛮力计算结合人类调整的启发式算法足以让国际象棋人工智能变得不可战胜，但围棋的高分支因子和全面战略框架使得使用人类预定义的规则或策略创建真正强大的人工智能成为不可能。

深度神经网络的模式识别和大数据为训练网络提供的巨大并行性都需要最终破解这个问题。

> 虽然训练神经网络所需的资源是巨大的(单台 PC 需要数千年的计算时间)，但一旦训练完毕，实际玩游戏所需的计算就少得多了。来自 DeepMind 的原文:*在与范辉的比赛中，AlphaGo 评估的位置比深蓝在与卡斯帕罗夫的国际象棋比赛中评估的位置少上千倍；通过更智能地选择这些位置进行补偿*

## 人类与机器的学习风格

DeepMind 关于 AlphaGo Zero 的论文提到了一些有趣的事情:人类初学者在第一次开始玩游戏时学习的梯子，是 AlphaGo Zero 在很久以后才在自己玩的游戏中消化的东西。不清楚这实际上是什么时候发生的。以下是 DeepMind 的论文对此的看法:*令人惊讶的是，shicho(“梯子”捕捉可能跨越整个棋盘的序列)——人类学习围棋知识的第一要素之一——直到很久以后才被 AlphaGo Zero 在训练中理解*

令人有点恼火的是，这篇论文没有说 Zero 花了多长时间才弄清楚梯子，但让我们假设它在最初的 3 天训练中已经进行了一半。如果是这样的话，Zero 的 ELO 应该已经在 3000 点附近了。这将使它跻身于世界前 500 名左右的职业选手之列。

这种现象非常重要，人工智能的整体性能非常高，但它对人类显而易见的东西有盲点。如果我们正在开发至关重要的人工智能应用程序，我们可能会认为我们的人工智能已经取得了非凡的性能，但它仍然容易偶尔犯一些相当小的错误。人工智能学习的方式，至少目前来看，与人类认知的工作方式有很大不同。

> DeepMind 在发表了他们关于 Zero 的论文后不久就让 AlphaGo 退役了，所以我们不知道还有哪些弱点可能会被发现为边缘情况。然而，目前有几个项目致力于实现 Zero 架构。这可能需要更长的时间，但最终我们应该能够更详细地研究一个与 AlphaGo Zero 实力相当的人工智能。

## 直觉和经验的重要性

DeepMind 关于 Zero 的论文中另一个让我印象深刻的是直觉和模式识别的重要性。在 DeepMind 的论文中，他们表明，一个只使用神经网络并且根本没有试图读出游戏中任何变化的 Zero 版本仍然具有大约 3000 的 ELO！这意味着，平均而言，它能够在专业水平上用纯模式识别来下围棋！

随着年龄的增长，我们快速精确计算的能力下降，但这向我们展示了经验的巨大力量。Zero 可以玩专业水平的围棋，远远超过几乎任何业余选手的水平，并且比许多从大约 8 岁开始全职训练的专业选手更好，根本不用读出任何序列。它只是用它的直觉来决定看起来最重要的地方在哪里。

对我来说，这真是令人震惊。我相信这也充分说明了智慧和经验对我们人类的重要性。

## 新手的心思

我想分享的最后一个观察是关于 Zero 和 Master 之间的力量差异，Zero 没有使用任何人类游戏进行训练，Master 具有相同的基本设计，但使用人类游戏进行训练。最终，Zero 在 Master 的改进似乎趋于平稳(略低于 5000 ELO)后继续显著改进。这表明人类的游戏抑制了师父探索一些有价值的想法。

事实上，Zero 彻底改变了我们对如何下围棋的想法。许多在过去会被人类专业人员立即关闭的游戏方式现在正以新的眼光被看待。作为人类，我们也必须对新思想保持开放，并努力推动自己进行更深入的探索，即使有些事情起初看起来似乎很愚蠢。

Zero 已经证明，即使是某个领域的顶级专家的公认智慧也可以而且应该受到质疑。我们应该始终以开放、好奇的心态对待任何问题，即使它挑战了我们先入为主的观念。

此外，如果我们在将人工智能应用于其他问题时使用人类专家产生的数据，那么值得记住这种限制。

## 结论和注意事项

在结束这篇文章之前，我认为有必要指出，尽管 DeepMind 在 AlphaGo 方面的成就令人惊叹，但与现实世界的开放式问题相比，围棋仍然是一个更容易解决的问题。围棋和国际象棋一样，是在有限的棋盘上进行的零和游戏:在游戏的最后总会有赢家和输家。围棋和国际象棋一样，也是一种[完美信息](https://en.wikipedia.org/wiki/Perfect_information)的游戏:两个玩家都可以进入同一个棋盘位置，没有任何隐藏或随机的东西(比如像扑克)。

在现实世界中，完全不可预测的事情可能会发生。以自动驾驶汽车为例，一名行人或另一辆汽车可能会在没有警告的情况下挡路。突发的天气事件可能会扰乱传感器。建筑工人可能会封锁一段道路。很难预测所有可能发生的事情，也很难预测人工智能应该如何应对。

还有法律和伦理问题:如果一辆汽车的人工智能不得不转向一些行人，潜在地伤害或杀死他们，以拯救司机，它应该这样做吗？如果有，制造商会被起诉吗？我认为在这种情况下起诉人类司机非常困难，但人工智能的行为是由软件调节的，可能受到不同法律标准的制约。

当然，还有黑客攻击的潜在问题。

我想底线是我们必须意识到现实世界有多复杂，并且不要太容易被人工智能在更受控制的环境中取得的成就所诱惑(无论这些成就可能多么令人印象深刻)。

## 链接

如果你对 AlphaGo 感兴趣，可以考虑看看:

*   [用深度神经网络掌握围棋游戏](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)
*   [在无人知晓的情况下掌握围棋游戏](https://deepmind.com/documents/119/agz_unformatted_nature.pdf)
*   [利用通用强化学习算法通过自我游戏掌握国际象棋和日本象棋](https://arxiv.org/abs/1712.01815)
*   [AlphaGo 纪录片](https://www.alphagomovie.com/)

DeepMind 的 AlphaGo Zero 架构的一些第三方实现:

*   [莉拉零点](http://zero.sjeng.org/)
*   [脸书精灵 OpenGo](https://research.fb.com/facebook-open-sources-elf-opengo/)
*   [Minigo](https://github.com/tensorflow/minigo)

如果您对更广泛的应用感兴趣:

*   [学习上网](https://online-go.com/learn-to-play-go)
*   珍妮丝·金的《学会下围棋》
*   [周边游戏电影](https://www.surroundinggamemovie.com/)