# 寻找大规模 Node.js 应用程序的性能瓶颈

> 原文：<https://dev.to/msedzielewski/hunting-down-large-scale-nodejs-application-performance-bottlenecks-lij>

这里有一个来自[视角](https://rspective.com)站点可靠性团队的案例研究。这次我们想分享一个客户的故事。为了降低基础设施的成本(每月 60，0 00 欧元以上)，我们的客户决定放弃 AWS，转而采用裸机+ kubernetes 解决方案。这种转移带来了预期的好处(托管数据库和缓存的机器更少，CDN 成本更便宜)，但出于未知的原因，它也突出了托管 Node.js 应用程序的机器上资源消耗增加和延迟增加的问题。

部分回归昂贵的 AWS 的威胁笼罩着整个企业。所以我们决定从内部仔细研究这个问题。

我们开始侧写。这是我们前进的第一步，也是第一个棘手的问题。一个在本地生成调用图和火焰图的应用程序在生产中无法工作。我们切换到手动 v8 概要分析，这意味着用`--prof`标志开始节点进程。

不幸的是，在节点版本 8.10 中，下载和处理日志失败。原因？一只虫子。8.12 同样的事情，好在 10.x 让我们继续前进。

我们分析日志来检查 CPU 峰值，从而找出占用处理器时间最多的部分。我们有一个疑点——是`lodash`的“查找”方法。我们优化它，这很有帮助。将数据格式从一个表转换为一个对象是一种补救措施，它会使几个端点的延迟增加 20-30 ms。

显然，我们还不满意。侧写会带来更多的嫌疑人。其中之一是影响后端处理的所有请求的一段代码。

事实证明，来自`lodash` - `cloneDeep`的另一个元素，本应提供不变性并在一年多前引入，但当前的数据量对处理器的延迟和消耗有负面影响。

这个问题很难捕捉，因为它对整体性能的影响一直在逐渐增长。正如通常在优化过程中发生的那样，长期寻找的麻烦在简单的改变后就消失了。在这种情况下，结果是用`Object.freeze`替换`cloneDeep`。

我们验证了 1 kubernetes pod 的补丁。结果，处理器消耗减少了 30%，整个供应的平均延迟从 140 毫秒减少到 30 毫秒。我们决定向所有生产机器推广。

最终效果看起来还算满意。在应用了补丁的 700 个单元中，平均处理器消耗从 30%下降到 8%，这意味着我们可以减少单元的数量。

通过逐步减少 100 个 pod 的批次，我们达到了 200 个 pod 的标记，CPU 消耗占峰值时间的 44%。这比 700 个吊舱的初始峰值时间(约 55%)要好。

我们取得了什么成就？我们已经释放了大量资源，并获得了空间来处理更多的流量和即将推出的功能。当然，客户也不必回到昂贵的 AWS。

*ICYMI——我们正在招聘[视角](https://rspective.bamboohr.co.uk/jobs/)和[凭证](https://www.voucherify.io/about)*