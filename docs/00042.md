# 你愿意参与创造这样的产品吗？

> 原文:[https://dev . to/yor ODM/would-you-willy-wanted-part-in-creating-a-product-like-this-247h](https://dev.to/yorodm/would-you-willingly-participate-in-creating-a-product-like-this-247h)

一个朋友告诉我关于这个网站的事。一开始我不相信她，后来推特上开始流传，我对人类未来的希望从我办公室的窗户掉了下去。更糟糕的是，我找到了[这张单子](https://github.com/daviddao/awful-ai)。

看一眼第二件物品:

## [](#discrimination)辨析

扫描你的面部并告诉公司你是否值得雇佣的应用程序。【[摘要](https://www.theladders.com/career-advice/ai-screen-candidates-hirevue)

基于人工智能的 Gaydar(同性恋雷达)根据一项新的研究，人工智能可以根据人们的面部照片准确地猜测他们是同性恋还是异性恋，这项研究表明，机器可以拥有比人类明显更好的“同性恋雷达”。【[摘要](https://www.theguardian.com/technology/2017/sep/07/new-artificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph)

种族主义聊天机器人——微软聊天机器人 Tay 花了一天时间向 Twitter 学习，并开始发表反犹太主义信息。

种族歧视的自动标签——一个谷歌图像识别程序将几个黑人的脸贴上了大猩猩的标签。

PredPol 是一个为警察部门设计的程序，用于预测未来可能发生犯罪的热点地区，可能会陷入过度监管黑人和棕色人种占多数的社区的反馈循环中。【[摘要](https://www.themarshallproject.org/2016/02/03/policing-the-future?ref=hp-2-111#.UyhBLnmlj)

COMPAS -是一种风险评估算法，由威斯康星州在法庭上使用，用于预测累犯风险。它的制造商拒绝透露专有算法，只有最终的风险评估分数是已知的。算法对黑人有偏见(甚至比人类还不如)。[ [总结](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) ][ [NYT 意见](https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html) ]

从你的面部推断犯罪行为 -一个从你的面部特征判断你是否是罪犯的程序。【[摘要](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/)

[iBorderCtrl](https://ec.europa.eu/research/infocentre/article_en.cfm?artid=49726)——对进入欧盟的旅行者进行基于人工智能的测谎测试(试验阶段)。考虑到每天有多少人穿越欧盟边境，很可能会有大量的误报。此外，面部识别算法容易产生种族偏见。【[摘要](https://gizmodo.com/an-ai-lie-detector-is-going-to-start-questioning-travel-1830126881)

Faception -基于面部特征，Faception 声称它可以揭示性格特征，例如“外向的人、高智商的人、职业扑克玩家或威胁者”。他们建立模型，在没有事先了解的情况下，将人脸分类，如恋童癖者、恐怖分子、白领罪犯和宾果玩家。[ [量词](https://www.faception.com/our-technology) ][ [视频音高](https://www.youtube.com/watch?v=x1QsDiWCV-o)

作为来自第三世界国家的软件工程师/开发人员/自由职业者，我和我的朋友经常开玩笑说“奥本海默因素”，意思是一个假设的场景，我们的工作被用来创建一些邪恶的软件。这份名单与此无关，这些人**愿意**参与撕裂我们留在这个地球上为数不多的美好事物。所以我问你，你愿意成为那些人的一员吗？为什么？。