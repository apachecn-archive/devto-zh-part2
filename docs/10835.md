# 超越人类专家的医疗保健:5 个有趣的案例

> 原文:[https://dev . to/andys 0975/health care-beyond-human-specialist-5-interest-cases-ed4](https://dev.to/andys0975/healthcare-beyond-human-specialist-5-interesting-cases-ed4)

On the anniversary of Chang Gung Memorial University this year, a medical seminar was held in April with the theme of "Application and Trend of Artificial Intelligence in Medical Care". From many speeches, it can be seen that the clinical practice of AI in Taiwan Province is already in progress, but the most resource-rich Chang Gung Memorial Hospital system hasn't kept pace. However, it is often heard that many doctors shout loudly when they come to the college to emphasize that AI can't replace doctors. Is this really the case?

At the end of last year, two medical centers, St Vincent's Hospital Melbourne and University Hospital Geelong, Australia, conducted the AI diabetic retinopathy screening [T0】 [1] 【T1] for the first time in the endocrine clinic. The sensitivity and specificity of the deep learning model used were all over 92%. During the trial period, about 100 diabetic patients participated, and they will receive the severity report that AI can complete on the spot and the ophthalmologist grading report that they can only receive two weeks later. The follow-up questionnaire shows that 96% of the subjects are very satisfied with the real-time report of AI, and even 78% of the subjects prefer the AI mode over the traditional manual mode. It can be seen that the impact of AI on doctors' business is not groundless. Even though no one knows how AI will reform the existing medical model yet, the development of its application will reveal some clues. Let's take a look at what interesting things AI may do beyond the limits of human doctors. They are not only creative, but also quite possible to realize.

## 1\. dynamic visual acuity cultivation of Endoscopic capsule robots.

With the development of electrical material science for many years, a swallowing capsule endoscope with camera and wireless image transmission device has been developed, and it is being tried to screen and diagnose gastrointestinal diseases, such as inflammatory bowel disease, ulcerative colitis, and colorectal cancer. Different from the standard endoscope [[2]](#ref2) , the endoscope capsule robot is non-invasive and painless, which is more suitable for long-term examination. Besides, it is small in size and has no pipeline restrictions, so it can explore the areas that the standard endoscope can't enter. At present, however, the moving force of capsule endoscope still mainly comes from gastrointestinal peristalsis. If remote control is to be carried out, accurate 3D position and angle real-time estimation (6-DoF pose) of robot is required. In recent years, some attempts have been made, including MRI, PET, ultrasound and other equipment, but many special sensor need to be installed on the capsule, which will reduce its mobility, and in many situations, it is impossible to correctly transmit and receive signals. Therefore, a method called visual odometry (VO) has become a new research hotspot.

Visual odometry, that is, visual ranging method, directly extracts features, estimates scale and depth from each frame of video film without other signals, and estimates the motion of the film sequence. Mehmet Turan et al. [[3]](#ref3) designed a deep learning model, Recurrent Cognitive Neural Networks (RCNs), to perform visual odometry. RCNN is a combination of RNN and CNN. CNN first learns the two-dimensional features of images, and then RNN is responsible for extracting the feature changes before and after the film sequence. The research team used an electromagnet-controlled capsule endoscope (MASCE) [[4]](#ref4) , photographed 5.3 ms per frame in five pig stomachs and a human model, and recorded the 6-DoF pose in real time with the gold standard Optitrack motion tracking system. A total of 80,000 ordered images were generated, and the 6-DoF pose could be learned and predicted in the RCNN model. In the future, the model can give the real-time 6-DoF pose prediction value without using the huge and complicated Optitrack system.

As can be seen from the following figure, among several methods tested by the team, the RCNN：EndoVO trajectory calculation proposed by the team is the closest to the truth. It can be seen that the well-trained AI can transform the visual field change of the capsule endoscope into its in-vivo movement dynamics, so that the remote controller can correctly evaluate how to move to the designated position. Even if it is to be used as a routine screening and diagnosis method in the future, remote control automation can avoid the quality instability caused by the difference of manual operation proficiency, and the ability to predict trajectory will help the automatic system to instantly correct posture.

[![](../Images/7e814e9c276b574a633be2c6bd8d3ccd.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--tcvvtZpO--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/mt4uwll6zfjnghuhor5e.png)

## 2\. Let da Vinci surgical robot know itself.

In recent years, minimally invasive surgical robots have swept the operating rooms of major medical centers, and they have achieved remarkable effects in many traumatic lesions that are difficult to handle with both hands [[5]](#ref5) . However, they are still semi-automatic and depend on doctors to operate the robotic arms themselves. Surgeons often complain that they can't achieve many purposes by hand feeling like general surgery. At the school anniversary seminar, Dean Xiu Chuan shared that in order to solve the problems of difficult positioning and accidental injury of important blood vessels, the hospital operating room introduced AR technology, and the mannequin established by CT or MRI was projected on AR glasses, so that doctors could finish their work more accurately and safely under the projection prompt when operating, and this is also an important step for the surgical robot to move towards automation. After all, the robot has no hand feeling, but its perspective positioning ability is better than that of humans.

However, when AR system is applied to surgical robots, there will be a problem. Robots may be able to see through the human body, but they don't know the surgical instruments. The existence of AR system may cause identification anomalies and errors, and it can't track the angle and distance of the instruments, which greatly reduces the practicality. In order to let robots "know" their own arms, the challenge held at MICCAI Conference last year was Robotic Instrument Segmentation [[6]](#ref6) , which provided a large number of first-view surgical operation films of da Vinci robots, required to complete the regional labeling (2) of various mechanical arms, and even could identify three parts (3) and seven types of instruments (4) on the instruments. As shown in the figure below, this challenge is quite difficult due to the uneven illumination angle and shadow position in the picture, as well as the untimely atomization of the photographic lens and visual interference such as blood contamination.

[![](../Images/afc9e53d50a33a977cac43146ce03255.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--glLjFRbp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/c6llsb9wxp0xhze030a8.png)

Two Kaggle masters won the championship in this challenge, and scored 0.887, 0.725 and 0.236 mean IoU in these three challenges, respectively. It can be seen that marking and distinguishing the types of instruments is the most difficult problem. The team tested a variety of CNN structures, namely the classic U-Net, and its variants TernausNet and LinkNet. The results are shown in the following table. If we don't consider the classification of instruments with poor results, TernausNet-16 with VGG-16 pre-training parameters performs the best, while LinkNet with ResNet-34 jumping structure has the fastest reasoning speed (90 milliseconds), which can complete the task of instrument segmentation in near real time. As for the part of the classification task, because some categories of instruments in the film appear relatively infrequently, it is possible to improve the classification performance by supplementing the data. Even if it can be seen from the accuracy that such tasks are still developing and need more data, once the goal is achieved, the full automation of surgical robots can be accelerated.

[![](../Images/49ca8732dc791b14326f92d7762ebe0b.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--UDF_2g8R--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/f3855zvod0mr6igx1873.png)

## 3\. Sand table deduction and upgrade from CT-only to MR-only.

The effect of radiotherapy depends on the spatial dose planning before the course of treatment. Traditionally, computed tomography (CT scan) is used to generate the 3D virtual structure of patients, and the CT intensity value (representing the degree of radiation attenuation) is converted into electron density for radiation dose calculation. However, the contrast of CT scan's soft tissue imaging is limited, which is not conducive to the outline of the target structure, and will cause radiation damage to some extent. Therefore, it has become a research hotspot to consider magnetic resonance imaging (MRI) instead of CT [[7]](#ref7) . Even though MRI has no radiation side effects and can provide better soft tissue contrast and timely spatial resolution, it has a fatal disadvantage that magnetic resonance signals are different from ionizing radiation signals and cannot be used to calculate the dose [[8]](#ref8) . If CT and MRI are performed at the same time, they can complement each other's shortcomings, but there will also be some problems such as high cost, cumbersome procedures and uncertainty of double image registration. Therefore, the academic circles hope to find an accurate and rapid MR-only method, which can generate the corresponding CT scan from MRI to calculate the dose, which is called pseudo-CT or synthetic CT (sCT).

In the past, the popular sCT method was atlas-based, in which MR images were distorted and registered with the standard MRI atlas, and then converted according to the CT values corresponding to the MRI atlas. However, when this method encountered anatomical variations or pathological differences, it would produce great errors. With the popularity of AI, the model-based method began to become the focus. The team of Utrecht Medical Center in the Netherlands developed a Conditional Deep Convective Generative Adversarial Network (CDC Gan) [[9]](#ref9) for the pelvic region, and generated sCT by pixel-to-pixel. GAN is the latest deep learning method, which is composed of a generator and a discriminator. The generator should try its best to make the discriminator distinguish the fiction from the real thing, while the discriminator should try its best to distinguish the fiction from the real thing. The two are against each other, so it is called. The team gathered 91 patients with prostate cancer, rectal cancer and cervical cancer, all of whom took CT and MRI at the same time, and registered them. The MRI protocol adopts Dixon Reconstruction [[10]](#ref10) , which photographs the in-phase spin echo images of water and fat once and the out-of-phase) images delayed by a few milliseconds of TE once. The two images can be combined into water-only or fat-only images. Because the CT values of water and fat are different, it is necessary to separate them to generate CT images.

In-phase, water-only, fat-only images are used as input, and cDCGAN is trained to generate sCT. Then, the model ability is evaluated according to the difference between real CT and sCT images and calculated dose. Results as shown in the figure below (IP = in-phase MRI), the model can generate a whole set of sCT from MRI within 5.6 seconds, and the mean absolute error (MAE) between sCT and CT is 61±9 HU, which has far exceeded the record [[11]](#ref11) (94.5 17.8 hu) of atlas method, and the dose distribution map calculated by sCT is only better than that of CT. It can be seen that even though the signal distribution is different, AI can still successfully convert cross modality images, so it is possible to simplify hybrid applications such as PET/MRI into MR-only in the future, which also lays the possibility of pharmacological MRI (phMRI).

[![](../Images/782ec94d4e78b128616334a71df6691f.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--3k-xbj0l--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/dh0fwb074rzoa3nnl5d9.png)

## 4\. Use a microscope to show the drug resistance of bacteria.

Antibiotic resistance has become a major threat to public health in this century, especially fatal to acute and severe patients with nosocomial infection. At present, antimicrobial susceptibility testing (AST) [[12]](#ref12) , such as broth macrodilution (BMD), is mainly used in clinic to calculate the minimum inhibitory concentration (MIC). However, this method takes several days, and requires high laboratory environment and operation technology, so it is often slow. Therefore, the academic circles are trying to use deep learning to speed up the process, hoping to save more lives as soon as possible and reduce the cost burden.

The University of Arizona team proposed DLVM-AST [[13]](#ref13) , aiming at the most common Escherichia coli with urinary tract infection and the resistance to five antibiotics (polymyxin B, Streptomycin, Ciprofloxacin, Aztreonam, Ampicillin). Antibiotics can cause dynamic changes in movement, morphology, cell division, etc. of drug-resistant people. For example, polymyxin B can reduce the moving speed, while aztreonam can lengthen the shape. The contaminated urine is mixed with high-concentration antibiotics or the same amount of water, and then introduced into a Microfluidic chip, which is recorded by an optical microscope. Then, the dynamics of individual bacteria in the film are independently superimposed into a static image as a dynamic track, which is input into the convolutional neural network, CNN) for training. The obtained model can calculate the proportion of bacteria that are not affected by antibiotics, and draw the inhibitory concentration curve to get the minimum inhibitory concentration. The results show that the accuracy of drug resistance discrimination of the model is as high as 87%, and the calculated MIC is very close to that of broth macrodilution. Its advantage lies in that it only takes 30 minutes to get the result, while the standard method can get it at the fastest time by over night. The author points out that in the future, if the consideration of biochemical dynamic characteristics, such as ATP consumption, protein and nucleic acid concentration, etc., will have the opportunity to improve the sensitivity and specificity. Perhaps in the future, ICU beds can automatically evaluate antibiotic resistance, make immediate prescription decisions, and reduce the burden of medical staff.

[![](../Images/4881aef2f32c4e51393b1228967b2b45.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--mB3bDYY5--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/s9edk876cdu7delyb7vs.png)

## 5\. Preventive health care on mobile phones

Preventive health care in daily life has always been a weak part of Taiwan Province's public health system. The main means are vaccination and disease screening, but all of them have to be carried out by the people themselves in places that provide services. For other uncomfortable aftercare on weekdays, there may be more experience in Chinese medicine. Even though traditional Chinese medicines have no side effects compared with western medicines, they all have their own biases, and it is still easy to hurt people if eaten by mistake or taken too much. Therefore, many mobile apps help people to understand concepts by importing opinions of traditional Chinese medicine practitioners and classic medical books, but they lack practical guiding significance, because people are still not sure whether their own situation meets the definition. Guangdong Provincial People's Hospital, which has the largest outpatient service in China, cooperated with South China University of Technology to try to design AI model [[14]](#ref14) from the most feasible tongue diagnosis among the four diagnosis methods, and learn how to give appropriate Chinese medicine recommendations according to the prescription medical record database. The principle is to use CNN to input a large number of tongue photo features with different angles, lighting, distance and distance, and mark the patient's prescription list as feature vector by Chinese Pharmacopoeia with Latent Dirichlet Allocation (LDA) unsupervised classification. As the training goal of CNN, the prescription made by analyzing tongue images by this model will be close to similar medical records in the past. However, TCM emphasizes the combination of four diagnoses. In order to increase practicality, the future team will continue to develop models that integrate other diagnostic methods.

On the other hand, Taiwan Province is located in the subtropical zone with a rainy climate, which is suitable for mosquito breeding. Even though mosquito-borne diseases such as malaria are almost extinct in Taiwan Province, Japanese encephalitis and dengue fever will still break out from time to time. Even though households are urged to change water regularly and keep utensils dry for many years, it is still difficult to avoid mosquito-borne diseases in outdoor and public places. In fact, in the 1940s, attempts were made to locate, count and even classify mosquitoes by the sound of their wings flapping while flying. However, because the radio equipment was expensive and difficult to popularize, it was not taken seriously. Today, the popularity of smart phones and the popularity of AI bring about the possibility of change. The flapping sound of mosquitoes varies not only with varieties, but also with gender, age, environmental temperature and humidity. However, it is basically impossible for human ears to distinguish the difference. Even if the sound is received by instruments, the mixing of background noises makes it difficult to separate the flapping sound signal of mosquitoes. Stanford University has done a mobile phone radio test [[15]](#ref15) , and the longest distance is about 10cm, and the background noise is below 50dB, so that it can be clearly recorded.

[![](../Images/2df83296e1ff794e892b30761c094401.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--Ri-f5_w1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/l2ia4f6484c41ttapmtc.png)

So the team at Oxford University tried to use deep learning [[16]](#ref16) to analyze the flapping sound of mosquitoes (including background noise) recorded by mobile phones to identify the existence of mosquitoes. The recording file is one-dimensional time series data, but compared with the background noise, the flapping wings of mosquitoes are unstable and periodic, and the amplitude is erratic, so it may be difficult to capture the pure flapping wings of mosquitoes directly by deep learning. Therefore, the original data of the recording file is preprocessed into the scale (frequency domain) × transition (time domain) two-dimensional data with wavelet transformation, which can distinguish the flapping wings of mosquitoes, and then CNN training is conducted. Results CNN has a good ability to identify mosquitoes, with AUC as high as 0.97, which is higher than that of trained humans (0.883 on average). In the future, if we can collect more kinds of wings-flApping recordings, it will be possible to make an app that can detect the existence of high-risk mosquito vectors around us one step earlier, so that people in endemic areas can be more alert, or see a doctor earlier.

# Summary

From the five interesting cases mentioned above, it can be found that AI can not only learn doctors' judgment on retinopathy, but also do many things beyond the reach of human bodies, increasing the possibility of medical care. In fact, there are more medical applications under research and development, and even products that have been approved for marketing by FDA in the United States. Perhaps in the future, many daily businesses of doctors and many professional jobs will be snatched away by AI, but isn't this a good thing? It's better for us to take a positive attitude towards the better quality of human life brought by scientific and technological progress, and at the same time explore our own initiative and innovation. Then even if the current doctors will be replaced, the new doctors in the future will be able to embark on their own path!

# References

1.Stuart Kee 等人(2018)“内分泌门诊糖尿病视网膜病变基于人工智能的新型筛查模型的可行性和患者可接受性:一项试点研究。”科学报告 8(1)

2.M. Sitti 等人(2015 年)“无线移动微型机器人的生物医学应用。”继续。IEEE 103(2)205–224。

3.Mehmet Turan 等人(2018)“Deep endo VO:一种基于递归卷积神经网络(RCNN)的内窥镜胶囊机器人视觉里程计方法。”神经计算，275，1861-1870

4.多安医学博士，西蒂医学博士。(2017)“用于细针抽吸活检的磁力驱动软胶囊内窥镜。”IEEE ICRA，第 1132–1139 页。

5.Burgner-Kahrs，j .，Rucker，D.C .，Choset，H. (2015)“用于医疗应用的连续体机器人:调查。”IEEE 机器人学报 31(6)，1261–1280

6.https://endovissub 2017-roboticinstrumentsegmentation . grand-challenge . org/

7.M.A. Schmidt，G.S. Payne (2015)“使用 MRI 进行放射治疗计划”Phys. Med。生物。60, 323-361

8.布朗等人(2014)“磁共振成像:物理特性和序列设计。”威利。

9.Matteo Maspero 等人(2018)“用于普通骨盆磁共振放疗的深度学习快速合成 CT 生成。”arXiv:1802.06468

10.狄克逊瓦特(1984)“简单的质子光谱成像放射学。”153(1), 189–94.

11.韩 X (2017)“使用深度卷积神经网络方法的基于 MR 的合成 CT 生成。”医学物理四月；44(4):1408-1419.

12.L. Barth Reller 等人(2009)“抗菌药物敏感性试验:一般原则和当代实践回顾。”临床传染病，第 49 卷，第 11 期，第 1749–1755 页

13.Hui Yu 等人(2018)“利用深度学习视频显微镜进行表型抗菌药物敏感性测试。”肛门。化学。，第 90 卷第 10 期，第 6314-6322 页

14.杨虎等(2018)“利用 CNN 和辅助潜在疗法主题从舌图像自动构建中药方剂”arXiv:202.02203 v2

15.Mukundarajan 等人(2017)“使用移动电话作为高通量蚊子监测的声学传感器”eLife6:e27854。

16.Kiskin 等人(2017)“用神经网络检测蚊子:深度学习的嗡嗡声”arXiv:1705.05180