# Postgres 中可重复读取与可序列化隔离级别的比较

> 原文:[https://dev . to/pgrm/repeatable-read-vs-serializable-isolation-level-in-postgres-56fo](https://dev.to/pgrm/repeatable-read-vs-serializable-isolation-level-in-postgres-56fo)

我想在这里回答的问题，是我在过去 1.5 年里的问题:

> 什么时候使用哪个隔离级别，在真实场景中有什么区别？

## 读犯怎么了？

首先，你可能会想，为什么我要比较这两个严格的隔离级别，而不仅仅是读提交。我认为在我的整个职业生涯和学生时代，我总是使用默认的事务隔离级别， *read committed* ，并且只在理论上了解其他的。(好吧，在我加入的一家公司，他们实际上也使用了 *read uncommitted* ，但我们先不说这个。)

那么，要回答这个问题，【read committed 隔离级别有什么问题？ -根据您的使用情况，可能什么都没有。但是，它确实会导致一些行为，这些行为可能会导致难以测试和修复的错误。简而言之，这些行为是:

1.  丢失更新- 2 事务读取旧状态；两者都执行更改；最后一次写入获胜；
2.  不可重复读取和幻像读取——当第二次执行相同的查询时，可能会得到比第一次更多或更少的元素；

是的，你可以阻止他们两个。您可以通过显式锁定来避免丢失更新，如果您只读取一次，则不可重复读取不是问题。但是这在您的应用程序中引入了额外的复杂性，这是我们不想处理的。

## 可序列化与可重复读取-文档

在阅读了[在线文档](https://www.postgresql.org/docs/current/static/transaction-iso.html)、 [Postgres wiki](https://wiki.postgresql.org/wiki/SSI) 甚至邮件列表之后，我仍然不太确定这些隔离级别之间的巨大差异。根据 SQL 标准，*可重复读取*可以允许幻影读取，而*可序列化*则不能。但是在 Postgres 中，他们在这方面没有区别。事实上，在 Postgres 9.0 之前，没有任何*可重复读取*隔离级别。[不信你去查文档。](https://www.postgresql.org/docs/9.0/static/transaction-iso.html)

随着 Postgres 9.1 SSI 的到来，旧的 *serializable* 隔离级别被重命名为 *repeatable read* ，一个新的、更严格的隔离级别出现了。[来自维基](https://wiki.postgresql.org/wiki/SSI):

> [...PostgreSQL 中的可序列化快照隔离(SSI)与普通快照隔离(SI)的比较。从 9.1 版开始，这些分别对应于 PostgreSQL 中的可序列化和可重复读取事务隔离级别。

我不会深入探讨 SSI 是什么以及它是如何工作的技术细节，主要是因为我不确定我是否完全理解了它。相反，这篇文章将关注两种隔离级别之间的实际差异，因为我在现实世界的应用程序中从*可串行化*切换到*可重复读取*时体验到了它们。

当您在隔离级别*可重复读取*(或*可序列化*)下运行事务时，您应该预料到事务会不时地失败:

> 错误:由于并发更新，无法序列化访问

然而，由于 SSI 和花哨的谓词锁(至少我认为这是它的实现方式)，Postgres 现在可以中止事务，但也会出现另一个错误:

> 错误:由于事务之间的读/写依赖性，无法序列化访问

如果你想了解更多关于 SSI 的知识，那么你绝对应该[查看 Postgres wiki](https://wiki.postgresql.org/wiki/SSI) 上的解释。在这篇文章的剩余部分，我将尽量坚持使用实际的例子。

## 一路可串行化

2017 年初，我们与其他几位联合创始人一起，开始为旅游行业构建 ERP 系统。因为我们之前经历过由于*更新丢失*问题导致的数据库不一致的痛苦，所以我们决定用稳定性换取性能(一开始)。想法很简单:

*   每个请求都被打包到一个可序列化的事务中，以保证并发请求不会破坏系统，我们不需要考虑这个问题。
*   一旦我们实际上有了很多客户，这就成了一个问题，我们将分析我们的选择并解决它。但是我们只有在遇到问题的时候才会考虑，而不是从第一天开始。

除了一个小小的例外，这一切都很好。还记得我之前提到的那些错误吗？在我们的 CI 构建过程中，我们还测试了并发请求，很快就开始出现很多*“由于事务之间的读/写依赖性，无法序列化访问”*错误。不过这并不意外，因为 Postgres 文档清楚地说明了在使用这两个隔离级别中的任何一个时，您都需要准备好重试事务。我们也可以很容易地以全球化的方式解决这个问题，不再去处理它。(实现细节，我们是如何做到的，将在一个单独的帖子里。)

### *现实世界中的读/写依赖关系*

我们在生产环境中使用*可序列化*隔离级别运行我们的应用程序将近一年。我们密切监视在此期间我们进行了多少次事务重试。我们的假设是，一旦我们有了更多的数据，情况会变得更好。我们假设的原因是**我们的理解**，SSI 应该如何工作。一部分是基于谓词锁，通过锁定索引页。如果您的表中只有几行(就像您在集成测试中所做的那样)，那么一个表中的所有数据很可能适合一个或几个索引页。这意味着，你很容易陷入僵局。这对我们来说似乎很好，因为在真实世界的场景中，你不会有很少的数据和很多的请求。事实上，我们几乎从来没有并发的写请求。您还应该记住，这些是内部重试，只会导致稍微长一点的响应时间。因此，即使在并发写入这种不太可能的情况下，终端用户也不会注意到任何事情。

在生产过程中，一切都按预期运行，我们也有一个集成环境。这个环境包含了无数拥有自己的测试账户和数据的合作伙伴。此外，我们开始为更大的潜在客户进行性能测试。所有这些导致了集成环境中的大量数据。那时我们意识到，随着数据的增多，情况并没有好转。我们试图提出一个新的模型来解释我们所看到的(正如我提到的，仅仅通过阅读 SSI 和 *serializable* 隔离级别的文档，很难理解我们用例的真实含义)。我们意识到大多数失败都发生在特定的请求过程中。此外，这几乎完全是*读/写依赖*错误，只有一小部分(少于`0.1%`)是*并发更新*错误。

### 阅读+插入表格成为瓶颈

到目前为止，我们在一个特定的表上有最多的冲突。从这个表中，我们首先读取数据，以便验证是否可以插入一个额外的行。我们为什么要这么做？想象一下跟踪库存。你不再计算你有多少物品，而是记录你买了多少，卖了多少。每次你想出售一件物品(想象它是一个网上商店)时，你都需要确认你还拥有它。如果其他人在此期间购买了它，您从表中读取的数据(您做出决策所基于的数据)会发生变化。万一你同时试图更新数据，Postgres 说这不再是可串行化的了。这就是为什么我们会收到这么多的*读/写依赖*错误。

这样做似乎是一个愚蠢的想法，因为保持计数是如此容易。然而，不是出售物品，想象一下你正在出租它们。例如，你可以租一辆车，这辆车可以在一天中的任何时间被取走，并在同一天或两周后归还。在这种情况下，很难计算出还有多少车可供租赁。相反，计算你已经出租了多少是微不足道的。

鉴于(我认为)Postgres SSI(*serializable*isolation level)的工作方式，通过改进该表上的索引结构来改进谓词锁定是可能的。然而，考虑的不仅仅是谓词，而是整个索引页面，还有一个额外的随机因素。为了把事情做好而进行调整似乎是一场艰苦的战斗，因此我们选择了一条不同的道路。

## 继续进行*可重复读取*

首先，我们切换了集成管道中的测试。我们现在可以轻松运行更多的进程，而不是局限于 2 个并发进程。我测试的最高值是 64 个并发代理攻击我们的应用程序。当然，构建代理的性能很差，但是它工作正常，几乎没有导致任何事务重试。所以我们的期望达到了。最后，我们有了一个模型，它很合适！

然而，问题很快就出现了。因为*可重复读取*不会抛出我们已知的*读/写依赖*错误，所以我们的 read+insert 表会有问题。我们想到了两种可能的解决方案:

1.  我们自己序列化访问。
2.  抱最好的希望，假设它真的不太可能引起任何现实世界的问题。

根据您的用例，选项 2(希望最好的)实际上可能是一个真正可行的选项。当然你可能会卖得有点多，但是订单也会被取消，所以也许不会有什么不好的事情发生。不幸的是，对我们来说，这不是一个可行的选择。相反，在少数情况下，我们必须手动序列化访问，并且比 Postgres 的 *serializable* 隔离级别更聪明。

### 手动序列化访问

你可能会想——这很容易，只需运行 [`SELECT [...] FOR UPDATE`](https://www.postgresql.org/docs/current/static/sql-select.html) 。这只在*读提交*隔离级别有效，因为...

*   ...对于*可重复读取*事务，这实际上是不必要的，因为它们无论如何都会因*并发更新*错误而失败。(然而，它确实使锁定更加积极，如果这是您所寻求的。)
*   ...*可重复读取*事务只能看到事务开始前提交的数据。因此，即使并发事务`T2`需要等待`T1`完成，在继续通过`SELECT [...] FOR UPDATE`锁定之前，它仍然不能读取新数据。(这实际上引起了另一个有趣的问题，我将在另一篇帖子中详细解释。)

请记住，显式序列化是在更新共享行时自动完成的。因此，如果您记录您的库存中有多少物品，除了存储购买和出售物品的事件之外，这个计数是一个共享资源，在一个事务中更新它会导致所有其他事务失败，并出现*并发更新*错误。如果您没有这样的共享行(即使它只是一个带有计算值的优化)，您可以创建一个。

以租车为例。虽然很难准确跟踪哪几天预订了多少辆车，特别是因为一辆车只能预订几个小时，因此每天会预订多次，但我们仍然可以使用该信息进行序列化。每次预订一辆汽车，我们就在一个单独的表中更新与预订相交的所有日期的计数器。包括第一天和最后一天，尽管我们不应该把它们算作一整天。这将足以触发*并发更新*错误，这种情况不太可能发生，即两个请求在重叠的时间段内(几乎)完全同时到来。如果这些时间段实际上没有重叠，因为我们没有考虑简单锁定表中的时间部分，这将只是一个误报，事务将重试，没有人会注意到任何事情。

## 结论

最后，我惊讶于 *serializable* 隔离级别对我们的效果。然而，如果你想要更多的可预测性，并且可以考虑读/写的不一致性，我倾向于使用*可重复读*。现在我知道了如何处理重试事务(博客文章将随后发表)，我不认为我想很快回到*读提交的*。当然，这很大程度上取决于您使用的 RDBMS，并且本文是 Postgres 特有的。如果您有一个高度并发的数据库，其中许多行是共享的，并且您不太关心多次读取数据时的一致性，那么 *read committed* 可能是您的最佳选择。但是默认情况下， *repeatable read* 看起来不错。