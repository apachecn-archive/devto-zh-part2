# 使用特斯拉 Autopilot 软件合乎道德吗？

> 原文:[https://dev . to/bosepchuk/is-it-ethical-to-work-on-the-Tesla-auto pilot-software-4lhh](https://dev.to/bosepchuk/is-it-ethical-to-work-on-the-tesla-autopilot-software-4lhh)

我对特斯拉自动驾驶汽车开发了解得越多，我就越担心作为特斯拉自动驾驶软件的软件开发人员的道德问题。让我解释一下。

### 特斯拉的超值报价

特斯拉正在向人们出售原型质量的 1 级或 2 级自动驾驶汽车。但它也在为承诺其汽车将通过未来的软件更新成为第三级汽车筹集资金。并且三级能力将比人类驾驶员安全至少两倍。当然，全自动驾驶汽车实际上还不合法。因此，这需要由每个管辖区的监管机构来解决(但埃隆·马斯克相信，一旦他证明自动驾驶比让人类驾驶安全得多，他们就会明白)。

### 假设多了去了

我认为特斯拉正在向公众和客户做出近乎欺骗的陈述。

这里有一个列表:

1.  特斯拉有可能在未来几年内制造出*三级自动驾驶汽车*
**   自动驾驶将比人类驾驶员更安全*   特斯拉将能够 ***证明*** 自动驾驶比人类驾驶员更安全*   当前的硬件将支持完全的自动驾驶能力*   北美各地的监管机构将允许自动驾驶仪在未经修改的情况下在他们的道路上使用*   特斯拉不会因为他们的产品质量被起诉*   特斯拉不会因为其他原因被迫进行公司终结召回或破产*

 *让我们浏览一下这些要点。

### 1。特斯拉有没有可能在未来几年内造出 ***安全*** 三级自动驾驶汽车？

这个论点有几个部分。

#### a)特斯拉 autopilot 软件需要完善

特斯拉不能指望它拯救的生命的功劳能抵消其自动驾驶汽车夺走的生命。自动驾驶汽车犯下严重错误或被怀疑犯下严重错误的那一刻，就会有大量的诉讼和负面宣传。当一辆自动驾驶汽车杀死一群孩子时会发生什么？

让我们做一些数学。据估计，自动驾驶汽车将包含大约 2 亿行代码。而行业平均水平是 15-50 个缺陷/KLOC。因此，如果自动驾驶汽车软件遵循行业平均缺陷率，我们可以预计每辆汽车中的软件包含 300 万到 1000 万个错误。作为一个社会，我们能接受这种情况吗？

#### b)达到全自动驾驶汽车的要求几乎是不可能的

我看不出特斯拉如何在短期内从其机器学习软件中获得完全自动驾驶汽车所需的精确度。我们在许多具有大量数据的产品中使用了机器学习，这些数据是比自动驾驶汽车简单得多的领域(滑动打字、语音识别、照片分类、语言翻译等)。)而且他们一直在犯错。在自动驾驶汽车软件中，你必须将多个深度学习系统链接在一起。每一部分都必须在几分之一秒内得出正确的结论，然后系统必须做出正确的决定，并能够执行它。

所有这一切都必须借助汽车中任何可用的计算能力来实现。它必须能够应对极端温度、硬件故障、来自宇宙射线的比特翻转、溅在传感器和摄像头上的泥浆、软件错误、机械问题、网络问题、网络攻击、新奇的路况、好斗和不可预测的人类、野生动物以及多辆自动驾驶汽车互动时的紧急行为(更多风险[此处](https://www.theregister.co.uk/2017/10/09/bugs_in_autonomous_vehicles/))。此外，它必须在汽车的生命周期内工作。考虑到所有这些需求，你希望我相信这些系统在 99.9999999%的时间里都能做好一切？

#### c)特斯拉的汽车更像原型车，而不是量产车

建造一辆没有激光雷达的自动驾驶汽车简直是疯了。那里。我已经说过了。我知道埃隆·马斯克认为他可以绕过它，但我可以向你保证，如果他的车也有激光雷达，会更安全。作为对比，Waymo 的汽车有 [3 种激光雷达，5 个雷达传感器，8 个摄像头](https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation)。你更愿意把孩子的生命托付给哪辆车？

但是还有其他问题。通用汽车和 Waymo 正在他们的汽车中构建[冗余系统，这样他们就可以应对故障情况而不会撞死你。特斯拉的冗余系统在哪里？](https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation)

任何开发或考虑购买自动驾驶汽车的人都应该阅读丰田意外加速案例。这里有一个精彩的[视频](https://youtu.be/NCTf7wT5WR0) ( [幻灯片](https://users.ece.cmu.edu/~koopman/pubs/koopman14_toyota_ua_slides.pdf))讲述了所发生的事情，以及为什么安全关键系统需要极高质量的工程流程和冗余。

#### d)你不能拿安全当儿戏

当你在开发一个一旦发生故障就可能导致死亡的系统时，你在构建一个安全关键系统。这与开发智能手机应用完全不同。你需要遵循一个类似于 ISO 26262 的流程。据我所知，[深度学习系统无法通过 ISO 26262 的认证](http://users.ece.cmu.edu/~koopman/pubs/koopman16_sae_autonomous_validation.pdf)，因为我们无法知道它们在每种情况下会做什么。它们不能受制于正式的方法。也不能对它们进行详尽的测试。这是一个大问题。

##### 与航空业的比较

如果你想看看严肃对待安全关键系统的例子，看看航空业。不允许深度学习。只有无与伦比的工程努力和质量保证。

[![Airbus A330](../Images/bad3f59d52495445a4f58ceb43da6323.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--rY20quyy--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/qski3jx8tentpgbf2zog.jpg)

例如，空客 A330 的飞行控制系统有 ***五倍冗余*** ！

亮点:

*   该系统同时在五台计算机上运行，只需要一台就能驾驶飞机
*   每台计算机都有两个处理器(称为通道),执行相同的计算，然后比较它们的结果
*   使用不同的处理器来减少制造或设计错误引入错误的机会
*   三台计算机组成主系统，两台计算机作为功能减少的备用系统
*   该系统包含四个版本的飞行控制软件，由独立团队使用不同的编程语言进行编程
*   所有传感器输入都是冗余的
*   所有执行器都是冗余的

这是对安全的严肃承诺([视频](https://youtu.be/EOexjozpBdI?t=1m55s)、[幻灯片](https://ifs.host.cs.st-andrews.ac.uk/Resources/CaseStudies/Airbus/Airbus-fcs.pdf))。对了，我上面说的 A330 系统是 1992 年引进的！ ***看看空客在 25 年前的所作所为应该会让你重新考虑特斯拉是否有业务建设安全关键系统*** 。

### 2。自动驾驶将比人类驾驶员更安全

航空业很久以前就发现自动驾驶技术并不都是有利的:

*   当飞行员使用自动驾驶仪时，他们的飞行技术就生疏了。
*   当自动驾驶仪意外断开时，他们很难重新获得情境意识。

特斯拉将面临这两个问题。

##### 法航 447 客机失事

法航 447 上的自动驾驶仪在收到来自传感器的错误空速读数后意外启动。**三名训练有素的飞行员，每个人都有数千小时的专业经验，并在那架特定的飞机上接受过广泛的训练，但他们不知道发生了什么**，忽视了他们电脑的警告，导致飞机坠毁，造成 200 多人死亡。

这对于普通车主来说可不是什么好兆头，因为他们从第一次拿到驾照起就没有接受过任何培训或学习。

顺便说一句，法航坠机事件并不是孤立事件。在自动驾驶系统突然断开后，飞行员通常很难获得态势感知。

想想看，几年后，当你有了一辆能为你处理这些日常任务的汽车后，你的夜间驾驶或高速公路通行技能会发生什么变化。这是自动化的悖论:你用得越多，如果意外失败，你的表现就越差。

顺便说一句，法航机组人员无法在 ***三分钟*** 内获得足够的态势感知，他们不得不对自动驾驶仪断开做出反应以拯救自己的生命。什么样的车会意识到自己无法提前三分钟处理好状况？没有。你在车里可能只有几秒钟的时间。让那件事过去一分钟。

##### 跳过三级自治

正是因为这个原因，[几家汽车公司已经决定完全跳过第三级 autonomy】。我相信](http://www.motortrend.ca/en/news/toyota-might-skip-level-3-autonomy/)[谷歌是第一个公开放弃第三级自治的想法](https://cleantechnica.com/2017/11/01/googlewaymo-stopped-testing-level-3-self-driving-tech-testers-literally-fell-asleep-using-switched-full-autonomy/)，然后其他人也跟着放弃。

### 3。特斯拉将能够证明自动驾驶比人类司机更安全

以任何有意义的方式证明你的自动驾驶汽车比人类更安全将是极其困难的。我不打算给你讲所有的统计数字。让我给你链接一篇[文章](https://www.greencarreports.com/news/1106613_how-safe-is-tesla-autopilot-parsing-the-statistics-as-suggested-by-elon-musk)，这篇文章很好地涵盖了问题的范围。这里我只谈几个重点。

[![graph](../Images/832e77efe2a36f4ae20931d3f541eb22.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--cPLCnQKv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/mv1afv2q6y3a7xq97eso.jpg)

##### 需要庞大的样本量

致命的车辆事故很少发生(在美国大约每 9400 万英里中有 1 起)，所以你需要一个巨大的特斯拉自动驾驶致命事故样本，才能相信你的平均死亡率具有统计意义(30 是一个粗略的经验法则)。在你可以声称你的自动驾驶汽车在统计上比人类司机更安全之前，有很多死亡和很多驾驶。

##### 更新或更改重置时钟

每次你更新软件或硬件，时钟都会归零，因为它基本上是一个新产品。

##### 你必须计算自动驾驶的净结果

你需要将三种间接死亡与无人驾驶汽车相提并论。

1.  由于人们开始依赖自动化而丧失驾驶技能，从而导致死亡。

2.  任何由自动驾驶仪/移交混乱造成的死亡。当自动化失灵时，人们很难控制汽车。但当你考虑到不同公司的自动驾驶汽车可能会以不同的方式处理相同的情况，或者同一辆汽车在软件更新后可能会有不同的行为时，情况会更糟。我预测这些问题会导致很多很多人死亡。

3.  无人驾驶汽车做人们意想不到的事情导致的死亡。在许多场景中，无人驾驶汽车可能会做一些人类意想不到的事情，导致第三方撞车。

换句话说，你需要的是全押的结果，而不仅仅是自动驾驶为特斯拉车主预防的死亡。

##### 需要严谨的研究来确定自动驾驶仪的安全性

你需要进行一项科学研究，以实际确定特斯拉的自动驾驶仪是否比人类司机更安全。你必须把所有想买带自动驾驶功能的特斯拉的人都找来，把车卖给他们，然后随机分配给他们，看他们是否能自动驾驶，直到每组至少有 30 起撞车事故，然后你才能提出任何关于安全的说法。

我过分简化了这个过程，但这就是它的要点。你不能只看所有车辆的总死亡率或事故率，并与特斯拉汽车的比率进行比较——这是苹果与橘子的比较。

因此，总而言之，汽车制造商必须获得批准，才能在公共道路上进行大规模的生死实验，以收集数据来证明自动驾驶汽车在公共道路上使用足够安全。并且每次硬件或软件的改变都会重置时钟。只有我一个人觉得有问题吗？

### 4。当前的硬件将支持完全的自动驾驶能力

考虑到我们甚至不知道制造一辆完全自动驾驶的汽车需要什么，我对此表示怀疑。

我给你讲个小故事吧。我在 20 世纪 90 年代购买了语音识别软件，因为我写了很多东西，制作该软件的公司向我承诺，它将会工作，并为我节省大量时间。你猜怎么着？它是如此不准确，以至于我几乎立即停止使用它。然而，该软件的每个版本都向潜在用户承诺，计算能力的进步和更好的算法已经解决了这些问题。谎言。事实上，20 年后的今天，我的声音已经在云端被难以想象的计算能力处理，相比我在 20 世纪 90 年代的台式机，语音识别仍然是断断续续的。

那么，同样的模式出现在自动驾驶汽车上的可能性有多大？如果在大多数情况下，从特斯拉目前的自动驾驶到一辆*安全的完全自动驾驶汽车，需要 100 倍、1000 倍或 10000 倍的计算能力(埃隆·马斯克暗示的能力水平)，会怎么样？*

 *### 5。北美各地的监管机构将允许自动驾驶仪在他们的道路上使用

如果你已经在为全自动功能存款，这是一个很大的假设。如果十年都没有批准呢？特斯拉能挺过去吗？但是还有其他问题。

不同的监管机构可能会对其道路上允许的自主程度以及自主使用的条件做出不同的限制。你能想象如果自动驾驶汽车根据其所处的司法辖区而具有完全不同的能力，会给每个人带来怎样的混乱和困惑吗？

监管机构还可以强制要求自动驾驶汽车在使用前满足某些要求。如果这些要求之一是激光雷达或遵守 [ISO 26262](https://en.wikipedia.org/wiki/ISO_26262) 或无单点故障，那会怎样？特斯拉不能声称任何这些东西。

最后，我想知道在发生特别可怕的事件或一系列事件后，监管机构是否有可能将自动驾驶视为非法。如果一辆自动驾驶汽车撞上一群学龄前儿童，会发生什么？

### 6。特斯拉不会因为他们的产品质量被起诉

特斯拉当然会被起诉。已经在被[起诉](https://www.consumeraffairs.com/news/lawsuit-charges-tesla-of-misleading-consumers-about-safety-of-its-autopilot-feature-022818.html)了。

[![lawsuits](../Images/cd4268f94585a1c32298896de3c71df8.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--qydNF_GX--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/2blrpje0y33uhm86biq9.jpg)

丰田已经为我之前提到的意外加速问题支付了超过 10 亿美元。虽然我不是法律专家，但我怀疑特斯拉比丰田暴露得更多。我预计原告律师在一些巨大的集体诉讼中会因为我在这篇文章中提到的所有原因而轻易地将特斯拉撕成碎片。特斯拉能承受多少诉讼？

### 7。特斯拉不会因为其他原因被迫进行公司终结召回或破产

大规模召回似乎是可以预见的。现在的硬件能支持全自主吗？监管机构会要求激光雷达吗？监管机构会要求特斯拉安装更多冗余系统吗？但是，即使这些事情都朝着有利于特斯拉的方向发展，想想自动驾驶汽车的新奇和复杂吧。大召回不是可以预见的吗？

由于其他原因破产是另一种可能性，因为[特斯拉是美国最被做空的股票](https://www.cnbc.com/2018/04/11/tesla-is-the-biggest-short-in-the-us-stock-market.html)。特斯拉能解决 model 3 的生产问题吗？如果未来几年特斯拉无法实现完全自主怎么办？监管机构会允许全自动驾驶汽车上路吗？如果通用、福特、Waymo 或另一家公司在特斯拉之前获得自动驾驶汽车的批准，会怎么样？人们会想要回他们的存款，这样他们就可以立即去买一辆自动驾驶汽车吗？如果监控自动驾驶比自己开车更难更累怎么办？

### 把所有的东西放在一起

我想回到我最初的问题:在特斯拉 autopilot 软件上工作符合道德吗？这里有一些引自[软件工程道德准则](https://www.computer.org/web/education/code-of-ethics)的话。

> 1.03.只有当他们有充分的理由相信软件是 ***安全的，符合规范，通过适当的测试，并且不会降低生活质量，减少隐私或损害环境*** 时，才批准软件。这项工作的最终效果应该是对公众有益。
> 
> 1.04. ***向适当的人员或机构******披露他们有理由相信与软件或相关文档有关的对用户、公众*** 或环境的任何实际或潜在的危险。
> 
> 1.06.公平公正 ***在所有关于软件或相关文档、方法和工具的声明*** 中避免欺骗，尤其是公开声明。
> 
> 2.01.*在自己的能力范围内提供服务，诚实坦率地面对自己经验和教育的局限性。*
> 
>  *3.10. ***确保对软件*** 和相关文档进行充分的测试、调试和审查。
> 
> 6.07. ***准确陈述他们所使用的软件*** 的特征，不仅要避免虚假的声明，还要避免可能被合理认为是推测性的、空洞的、欺骗性的、误导性的或可疑的声明。
> 
> 6.10. ***避免与本规范*** 相冲突的企业和组织发生关联。*

 *【所有重点都是我的。]

当然，我没有第一手资料表明特斯拉发生了任何不道德的事情。但是假设我在这里展示的东西有一半是真的。使用特斯拉 autopilot 软件合乎道德吗？

#### 人员配备问题

特斯拉已经多次报道了自动驾驶团队主要成员离职的消息。这里见[，这里](https://www.reuters.com/article/us-tesla-moves-lattner/teslas-autopilot-software-head-quits-in-less-than-six-months-idUSKBN19C0B5)，这里。人们离开自动驾驶团队是出于道德原因吗？

然后是埃隆·马斯克的这条推特:

> 我们正在寻找核心软件工程师。不需要以前的汽车经验。请包括代码样本或链接到您的工作。

如果这些工程师没有汽车或航空航天方面的经验，他们是否有违反原则 2.01(在能力范围内提供服务)的风险...)?

### 最终辩词

我非常担心特斯拉正在发生的事情。制造一辆安全的自动驾驶汽车将会非常困难。任何人都不应该为了更快进入市场或获得更多利润而偷工减料。特斯拉自己称其软件发布为“测试版”。我不知道你怎么能在公共场合测试未经训练的客户作为良心的司机。至少我希望看到无人驾驶汽车像[这样接受测试。](https://www.eetimes.com/author.asp?section_id=36&doc_id=1333143)

#### 航空业绝不会以这种方式开发新技术

你认为波音或空客可以在载客飞机上测试深度学习自动驾驶系统吗？不太可能。如果他们声称它会比他们目前的自动驾驶系统安全两倍呢？无关紧要。你能想象如果一架客机因为测试版软件故障而坠毁并导致机上所有人死亡会引发多大的火灾吗？那么，我们怎么能在汽车上做这些呢？

#### 埃隆马斯克(Elon Musk)的自动驾驶汽车方法注定要失败吗？

本月早些时候，埃隆·马斯克[在推特上对 model 3 生产线的问题做出了如下回应:](https://twitter.com/elonmusk/status/984882630947753984)

> 是的，特斯拉的过度自动化是一个错误。准确地说，是我的错误。人类被低估了。

我提出这个问题有两个原因。首先，埃隆·马斯克(Elon Musk)已经赢得了几乎在任何房间里都是最聪明的人的名声。但是他确实会犯错误。我们不应该盲目相信他的判断。其次，我想知道在接下来的几年里，他是否会被迫承认与特斯拉自动驾驶系统完全相同的事情。再读一遍那条推文——它会完美地工作。

[![car crash](../Images/375ab0dc83b44479085af25767dada55.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--6te266-d--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://thepracticaldev.s3.amazonaws.com/i/hvt60z5d1uk384yh46oc.jpg)

### 底线

我完全支持新技术，让道路更加安全，但特斯拉的做法似乎有些鲁莽和不道德。如果你一定要叫我老顽固，那就叫我老顽固吧，但我想要的是特斯拉自动驾驶软件，它是像现代空客喷气式飞机软件一样设计、工程、构建、测试、验证和支持的，而不是由埃隆在 Twitter 上吹捧的任何软件开发商尽快构建的智能手机应用的错误原型。

*同意或不同意。我很想听听你的想法。*

喜欢这篇文章吗？请在下面“点赞”。***