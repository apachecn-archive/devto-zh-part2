# 发展风格:大 O，排序

> 原文：<https://dev.to/micahshute/developing-style-big-o-sorting-13pj>

在我学习编程基础时，我的研究文档的继续。欢迎反馈。

# 大魔神

## 什么事

它描述了输入增长时系统的渐近性质。通常，我们描述的是算法的运行时间，但它也可以用来描述空间复杂性，甚至是计算机科学领域之外的系统。这里我假设它描述了一个算法的运行时间，除非另有说明。渐近分析意味着你关注的是算法的运行时间是如何随着输入的增长和趋近于无穷大而增长的。该输入通常表示为`n`。随着我们的数据集变得越来越大，增长函数将成为运行时的主导因素。例如，O(n)表示运行时间随输入`n`线性变化。O(n^2)提出，运行时间与输入平方的大小成比例地变化。对于大型数据集，这些信息通常足以告诉您哪一个会更快。

当您分析一个函数的渐近特征时，您希望忽略添加的常数，以及下降系数和低阶项。例如，f(n) = 100 * n * lg(n) + n + 10000 被描述为 O( n * lg(n))，因为随着 n 趋于无穷大，常数和系数变得无关紧要。查看这些图表:

[![Runtime small](../Images/8d55dec0cde96cfd588d2409c2c3445a.png)](https://res.cloudinary.com/practicaldev/image/fetch/s--Xy-cnaq4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.imgur.com/JTjq1EC.png)
[![Runtime large](../Images/994349bff3e90231ee86e22cf9cb3fa1.png)T6】](https://res.cloudinary.com/practicaldev/image/fetch/s---4PCocuC--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.imgur.com/ygPiblV.png)

如你所见，在小数据集中，大 O 符号可能不能说明全部情况，但即使从上图中的 50 到 500 个数据点，函数的渐近性质也会起作用。

当然，在一定的渐近运行时间内优化代码以最大化性能是有好处的，但是只有在使用了正确的渐近算法之后才应该这样做。

虽然通俗地说“big-O”似乎是用来表示渐近运行时间，它描述了函数的“紧渐近”性质，(即描述了上限和下限)，但这实际上是不准确的。渐近运行时间的正式定义简述如下[1]:

*   大-θ(θ):

    *   形式上，对于描述函数 f(n)的θ(g(n))，存在正常数 c1、c2 和 n_o，使得`0 <= c1 * g(n) <= f(n) <= c2 * g(n) for all n >= n_o`
    *   **澄清:**假设你有一个由复方程 f(n)定义的算法。你可以用一个非常简单的方程 g(n)来描述 f(n)的 big-theta 符号，这个方程去掉了所有的常数、系数和低阶项，如果你可以在 g(n)上再加两个任意常数，就可以把原始方程 f(n)夹在中间，超过一定的输入数据大小 n_o。
    *   例子:假设你的 f(n) = `100*n^2 + 4*n*lg(n)`。我们可以用下面的等式和图表证明 n^2 符合这个定义

    [![Bounded](../Images/966f1552153ca05863c476ecfb1cfdc9.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--V9sY4Dcu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.imgur.com/0NIB2lH.png)

    *   现在，如果我们试图将 f(n)与 nlgn 夹在中间，我们会看到，随着 n 的增长，f(n)不可避免地会超过上界，而不管我们设置的常数是多少:

    [![Not bounded](../Images/dfdad1eadbbbed617067ad229f3019b0.png)T2】](https://res.cloudinary.com/practicaldev/image/fetch/s--axPUlXgp--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://i.imgur.com/4wEwjhM.png)

    *   所以，技术上我每次调用上面的函数 big-O，我应该说的是 big-theta，即θ(n)而不是 O(n)。
*   大 O:

    *   仅提供渐近上界，而不是θ符号提供的上下界。
    *   形式上，对于描述函数 f(n)的 O( g(n))，存在正常数 c 和 n_o，使得`0 <= f(n) <= c*g(n) for all n >= n_0`
    *   所以说一个由`f(n) = 3*n + 100`定义的算法是 O(n^3)实际上是正确的，尽管它是θ(n)。
*   还存在类似于上述定义的其他较少使用的符号:

    *   ω(大ω)符号仅定义了一个渐近下限
    *   o (little-o)符号定义了一个非渐近紧的上界(例如上面的例子，我们将θ(n)函数描述为 O(n^3)——这是一个渐近非紧的定义，因此你可以说它也是 o(n^3)。o(n)是不正确的)
    *   ω (little-omega)符号定义了一个一定不是渐近紧的下界。

在描述算法时，使用最多的概念是θ符号，通常用 O 符号来表示。

尽管算法可以按最佳情况、一般情况和最差情况运行时进行分析，但我们通常(除了下面的几个例外)关注最差情况，原因有三:

1.  计算最坏情况下的运行时间比计算平均运行时间更容易(需要考虑的因素更少)
2.  最坏情况的分析是一个保证，我们永远不会再花时间了
3.  实施时，最坏的情况经常发生，这并不罕见

事实证明，lgn(n 的以 2 为底的对数的简写)对很多运行时讨论都很重要。对数的快速回顾:

*   对数描述了为了产生一个特定的数，必须以什么为底的幂。例如，由于我们的基数是 2，如果我们想找到 lg(8)，我们问自己:“为了获得答案 8，我可以将 2 乘到什么次方？”。在这种情况下，因为我们知道 2^3 是 8，我们知道 lg(8) = 3。你正在“解构”一个指数。这是一个重要的概念，因为如果我们能做出一个由日志定义的算法，它会增长得非常慢！它实际上增长得和指数增长得一样慢，这是很多的。
    *   假设您有一个大小为 262144 的数据集，您正在执行一个运行时为 f(n) = n 的算法。如果增长到 524288，你就是在多做 262144 的操作！如果你的 f(n) = n^2，你就是在做 68719476736 多操作！如果你的 f(n) = lgn，你就多做了 1 次运算。如果你的 f(n) = lgn，你的数据集每翻一倍，你就多做 1 次运算。

## 为什么重要？

当您处理大型数据集或随着用户增加或收集更多信息而变大的数据集时，您的运行时间将由其渐近增长决定。如上所述，与最高阶分量相比，常数、系数和较低阶项具有相对较小的重要性。一个 O(n*lg(n))算法和一个 O(n^2)算法之间的区别很容易就是一个性能良好的应用程序和一个绝对不可用的应用程序之间的区别。

## 你是如何实际使用 O-notation 的？

*   理解什么时候重要
    *   如果您正在处理一个小数据集，并且它绝对没有办法变大，那么可能不值得您花费时间来开发超级优化的代码或尝试为您的算法获得更好的运行时。
*   如果您的数据集非常大或者可能变得非常大，了解您的算法如何影响您的运行时(和您的内存)是非常重要的！).因此，你必须能够知道你的代码的渐近运行时间是多少。一些方法可以做到这一点:
    *   看看你的循环。如果你正在遍历一个集合，你很可能有一个θ(n)算法。如果你的循环有一个遍历所有数据(或者每次 n-1)的内循环，你很可能有一个θ(n^2 算法，这是非常危险的。
    *   理解当你调用一个不是你自己写的或者是语言本身的函数时出现的“幕后”代码。可能有更好的方法来做某件事，如果不理解您所调用的方法的本质，优化起来会非常困难。
    *   仔细检查代码中与输入数据大小相关的函数，忽略与数据大小无关的常量或重复代码。这可以帮助你更容易地确定你的θ(g(n))
*   采取**分而治之**的心态
    *   (1)将问题分成更小的问题，(2)递归地征服子问题，直到它们小到足以有一个简单的解决方案。(3)组合子问题解决方案以创建整体解决方案。
    *   基本上，当使用分治法时，如果你能把你的问题分成相等的两半(这通常是分界点)，直到它足够小，以至于很容易解决。然后你可以解决这个小而简单的问题，并将这些简单的答案组合成一个完整的答案。很有可能你刚刚把你的算法从θ(n^2)改为θ(nlgn)运行时。
    *   发生这种转换的原因是，如果你把你的代码分成相等的两半，你可以做 lgn 次除法(即，我可以乘以 2 多少次来得到我的数据大小 n？).每次你把你的问题除以 2，我们迭代通过我们的数据做一些事情，因此 n * lgn。下面的排序算法将显示这种情况的实例。

# 排序

这是大 O 的一个简单应用，也是每个程序员都应该基本了解的。

## 为什么重要？

大多数语言都有内置的排序算法，但是理解排序是如何工作的仍然很重要。为什么？

它有许多重要的使用案例:

*   可以说最大的用途是将搜索数组的速度从 O(n)提高到 O(lgn)——这是一个巨大的进步。(即使用[二分搜索法](https://en.wikipedia.org/wiki/Binary_search_algorithm)的能力)
*   以某种格式(价格从高到低)向用户交付数据
*   基于数据在排序列表中的位置操作数据(即显示最近的帖子)

此外，虽然我自己从未经历过，但你显然可以在技术面试中被问到这个问题。

根据运行排序算法的数据，它们的运行时间可能会有很大的不同。虽然许多给定的排序算法在大多数数据上工作得很好，但了解哪些不同的算法可能更适合您的用例是很重要的。简单地使用该语言的标准排序算法可能不是最佳选择。

最后，也许是最重要的，它帮助你理解为什么大 O 很重要，以及如何以一种比暴力解决方案更优雅的方式解决问题。

## 怎么做，用什么时间复杂度？

### 免责声明:

这些例子都集中在时间复杂度上，空间复杂度就不考虑了。

此外，这些算法的更好的度量是它们进行的切换和比较的数量，而不是特定机器上的运行时间。为了方便、直观和清晰，我将提供运行时。我的 RAM 从来没有被完全使用过，磁盘 IO 也没有考虑到运行时。

在方法名中用`!`标注的算法就地排序。

## 算法

### 冒泡排序

*   特征
    *   θ(n^2)最坏情况和一般情况
        *   注意嵌套循环给出了θ(n^2)运行时的指示
    *   θ(n)最佳情况
*   描述
    *   遍历列表，检查每个元素及其相邻元素。如果它们坏了，就把它们换掉。重复此操作，直到全部交换完毕。作为一种优化，你可以每次迭代`n`减去`i`个元素，其中`i`是你已经完成的迭代次数。这是因为在第一次扫描之后，您保证最小的数字已经“冒泡”到顶部(或者最大的数字已经下降到底部，这取决于您的迭代方向)，所以您只需要检查它下面(上面)的索引。
    *   冒泡排序的 wikipedia 页面显示了一个更简单的冒泡排序实现，并逐步发展到“优化”版本。
    *   即使经过优化，冒泡排序在大型数据集上也非常慢。如果没有交换，你可以把它编程为`return`，这意味着数据是有序的，但是这只有在你有一个几乎完美排序的数组时才有帮助。为了清楚起见，我没有包括这种优化，因为插入排序比冒泡排序更好地利用了这种特性。
*   密码

```
def bubble_sort(coll)
    for i in 0...(coll.length - 1) do
        for j in (i+1)...coll.length do
            if coll[i] > coll[j] 
                coll[i], coll[j] = coll[j], coll[i]
            end
        end
    end
    coll
end 
```

### 插入排序

*   特征
    *   θ(n^2)最坏情况和一般情况。
    *   θ(n)最佳情况
        *   这种最佳情况发生在列表已经排序(或者非常接近排序)的时候
*   描述
    *   想象你的收藏被分成两部分。最左边的索引(即索引 0)是一个元素的“排序数组”。数组的其余部分是一片混乱。我们可以在未排序的右边一个接一个地进行，并在数组的左边正确地插入该元素，以保持左边的排序，并使排序后的边增加 1。当我们到达数组的末尾时，整个左侧(即整个数组)都被排序了。
    *   这是θ(n^2 ),因为对于我们未排序的一边的每个索引，我们必须在已排序的一边搜索它的正确位置。不仅如此，我们还必须在途中移动每个元素，为我们的插入腾出空间，这也是很昂贵的。
        *   重要的是要注意比较的次数和交换的次数。
    *   对于数组右边的每个索引，我们从 1 到左边的其他元素寻找正确的插入位置。1 到 n 的求和是θ(n^2 量级的)
    *   我们可以通过一个“循环不变量”来评估这个算法
        *   循环不变量是证明执行重复任务的算法的正确性的一种方式。为了正确地做到这一点，我们必须给出一个*循环不变(LI)语句* ***正确的*** 为:
            *   初始化——在第一次循环之前，LI 语句为真
            *   维护-您的 LI 语句在每个后续循环期间都是正确的
            *   终止——你的 LI 语句在程序的末尾，给出了一种方法来“证明”你的算法是正确的。
        *   在这种情况下，循环不变量可以表述为(转述自[1]):“在我们的 for 循环的每次迭代开始时，从 0 到 j-1(包括 0 和 j-1)的子数组由最初在该索引范围内但现在已排序的元素组成”
            *   **初始化**:因为左侧是单个元素(即 0 到 j=1-1=0 包含在内)，所以按定义排序。
            *   **维护**:每次循环时，将一个物品从未分类侧插入到分类侧的正确位置。然后你更新你的指数，这样你就知道这个分界线在哪里。你的排序边大了一个，仍然是排序的。在循环之前和循环之后，子数组 0 到 j-1 被排序，但是在循环结束时，j 比之前大一个单位。
            *   **终止**:当 j 等于你的数组长度减 1 时，你的循环结束。因此，我们知道从 0 到(数组长度- 1)的数组是有序的。这是数组中的所有索引。我们的整个数组都排序了。
    *   正如您将在下面看到的，虽然大多数时候插入排序非常慢，但它是排序几乎已排序数据的明星。
*   密码

```
def insertion_sort!(coll)
    for j in 1...coll.length do
        key = coll[j]
        # insert coll[j] into sorted sequence coll[0..j-1]
        i = j-1
        while i >= 0 && coll[i] > key
            coll[i+1] = coll[i]
            i -= 1
        end
        coll[i+1] = key
    end
    coll
end 
```

### 合并排序

*   特征
    *   θ(n * lgn)最佳情况、最差情况和一般情况
*   描述
    *   分治算法:
        *   将问题一分为二，直到数组大小为 1
        *   **克服**当大小等于 1 时的排序问题——因为只有一个元素，所以这个问题已经解决了。这将给你从`merge_sort`函数的第一次返回，而不是一次递归。这个返回将总是一个排序的数组(长度为 1 或进入排序数组的长度为 T1 ),并允许你继续这个算法，只要在你`combine`它们时保持这些数组的“排序”状态。
        *   **通过比较将你的两个尺寸都为 1(并且排序)的较小的一半组合在一起，形成一个长度为 2 的排序数组。**
        *   向上一级递归现在将有 2 个大小为 2 的数组，它们被排序为 T1。 ***将*** 组合在一起，创建一个新的长度为 4 的排序数组，依此类推，直到你的整个数组排序完毕。
    *   **合并循环不变量** [1]，(参考下面的代码):
        *   语句:在 while 循环的开始，`merged_arr`将包含数组`left`和`right`中最小的(i + j)个元素，按照排序的顺序。对于大于 I 和 j 的索引，`left[i]`和`right[j]`将分别在它们各自的数组中包含最小的数。
        *   **初始化** : `merged_arr`为空，i + j = 0。`left`和`right`是排序后的数组，所以它们的第 0 个元素都是整个数组中最小的。
        *   **维护**:将`left[i]`和`right[j]`两个数中较小的一个追加到 merged_arr 的末尾。由于两个数组都是有序的，并且 I 和 j 都从 0 开始递增，因此我们保证 while 循环的每次迭代只考虑追加到`merged_arr`的大于或等于已经追加到`merged_arr`的数字，并且小于或等于`left`和`right`中剩余的更高索引的数字。这保证了`merged_arr`的排序条件。当一个元素从`left`追加到`merged_arr`时，只有`i`增加 1，当一个元素从`right`追加到`merged_arr`时，只有`j`增加 1。这确保了`merged_arr.length`等于 i + j，`merged_arr`的值是有序的，并且接下来的两个数字`left[i]`和`right[j]`大于或等于`merged_arr`中的所有数字，并且小于或等于它们各自数组中剩余的所有其他值。也就是说，它们是它们的数组必须提供的两个最小值，还没有放入`merge_arr`。
        *   **终止**:当其中一个数组为‘空’时，循环中断。我们知道在这一点上`merged_arr`是有序的，并且具有扩展数组的所有值和另一个数组的所有值，直到索引`i`或`j`。这种终止是伟大的，但只需要一点点清理。现在，您必须将数组的剩余值推到`merge_arr`的末尾。你知道你可以把它们推到最后，因为有剩余值的数组已经排序了，并且它的最小值大于或等于另一个数组中的最大值。
            *   注意，有时两个“哨兵”被放在`left`和`right`的末尾，并被赋予无穷大的值。然后，while 循环被一个运行了`left.length + right.length - 2`次的 for 循环取代，这样就只剩下两个无穷大的标记值了。在 Ruby 中，你可以用`Float::INFINITY`来表示无穷大，在 Python 中`float("inf")`，在 Javascript 中`Infinity`，在 Java 中`Double.POSITIVE_INFINITY`。
*   代码(取消注释以查看它的运行——这有助于了解排序是如何工作的)

```
def merge_sort(coll)
    len = coll.length
    return coll if len == 1
    left = coll[0, len / 2]
    right = coll[len / 2, len]
    left_sorted = merge_sort(left)
    right_sorted = merge_sort(right)
    merge(left_sorted, right_sorted)
end

def merge(left, right)
    merged_arr = []
    i, j = 0, 0
    while i < left.length && j < right.length
        #puts "Comparing lhs #{left[i]} to rhs #{right[j]}"
        if left[i] <= right[j]
            #puts "Adding lhs #{left[i]}"
            merged_arr << left[i]
            i += 1
        else
            #puts "Adding rhs #{right[j]}"
            merged_arr << right[j]
            j += 1
        end
    end
    merged_arr.concat right[j, right.length] if i == left.length
    merged_arr.concat left[i, left.length] if j == right.length
    #puts merged_arr.to_s
    merged_arr
end 
```

### 快速排序

*   特征
    *   θ(n * lgn)最佳情况，一般情况
    *   θ(n^2)最坏情况(当接近排序时)
        *   如果您查看下面的代码，您会发现这是因为在排序的情况下，透视将问题分成两个大小 n - 1 和 1，而不是 n/2 和 n/2。对于随机化的数组，更有可能的是，中枢将是更接近中间值的值。这将在下面详细描述。
*   描述

    *   **分而治之**
        *   **Divide** 选择一个“轴心点”作为数组中的最后一个索引。遍历数组并切换位于 pivot 值“错误一侧”的值，直到所有大于或等于 pivot 的值都位于某个索引的右侧，而所有较小的值都位于同一索引的左侧。将数组除以该索引，得到两个更小的子问题。
        *   **征服**当尺寸足够小时(即 1 或 2)，这个**划分**过程将减少要排序的子集。在大规模上通过大于或小于一个枢轴来对数字进行分组确保了在较小规模的 2 的阵列上完成的算法(其实际上导致阵列的排序)是在阵列中的某个位置完成的，该位置将导致整个阵列被排序，而不仅仅是具有局部排序数据的块。
        *   **合并**就像合并排序算法的**征服**一样琐碎，因为所有的工作都是在合并排序**合并**中完成的，快速排序的**合并**是琐碎的，因为所有的工作都是在**除**步骤中完成的。在数组被分成 1 和 2 的集合后，它被就地排序。并且当子阵列的大小变得等于或小于零时，算法将停止递归。
    *   **分区循环不变量**(参见下面的代码)
        *   声明:`arr`的值从原始参数值`lo`到当前值`lo`都小于等于枢纽值。从原始参数值`hi`到当前值`hi`的`arr`值大于或等于枢纽值。
        *   **初始化** `lo`和`hi`分别被初始化为比它们的初始值小 1 和大 1。由于在数组之外，并且不与值相关联，您很容易满足这样的要求:`arr`的相关值如 LI 中所述。
        *   **维护**在一次循环后，`lo`索引值递增，超过任何小于枢轴值的索引。当循环最终中断时，`lo`因此被保证固定在一个索引上，该索引的值大于或等于 pivot 值，它左边的所有值被保证小于 pivot 值(或者等于 pivot 值，如果它被交换到`lo`的左边)。‘hi’以同样的方式递减，停止在其值小于或等于 pivot 值的索引上，保证‘hi’右边的所有值都大于 pivot 值(或者等于 pivot 值，如果它被交换到`hi`的右边)。如果`hi` > `lo`，我们交换它们，因为我们知道内部循环停止在应该与数组的‘另一边’或‘中间’除以主元值相关联的值上，而我们还没有到达‘中间’。这意味着`arr[lo]`的新值必须满足`arr[hi]`回路断开的要求(即`<= pivot`)，并且`arr[hi]`的新值必须满足`arr[lo]`回路断开的要求(即`>= pivot`)。因此，保持在索引`lo`和以下的所有值或者小于中枢(如果它们被循环传递)或者小于或等于中枢值(如果它们从先前在索引`hi`的点交换)。换句话说，它们都小于或等于 pivot 值。相反，认为索引“hi”和更大的所有值要么大于中枢值，要么等于中枢值。
        *   **终止**如果`lo` > = `hi`，我们知道我们已经到达了数组中的位置，该位置在数值上被 pivot 值所除。我们返回两个索引中较高的一个，允许我们创建一个数组的细分，其中所有低于`partition_index - 1`的值都小于或等于主元，所有高于`partition`的值都大于或等于主元。同样，通过像在`quicksort!`中那样细分数组，我们通过防止分区索引细分为空数组和原始数组来防止无限循环。当子阵列的大小为 1 或 2 时，`partition`产生一个排序的子阵列部分，它也在相对于整个阵列的正确位置。
*   代码

```
def quicksort!(coll, lo = 0, hi = nil)
    hi = coll.length - 1 if hi.nil?
    if lo < hi
        partition_index = partition(coll, lo, hi)
        quicksort!(coll, lo, partition_index - 1)
        quicksort!(coll, partition_index, hi)
    end
end

#Hoar Partition
def partition(arr, lo, hi)
    pivot = arr[hi]
    lo -= 1
    hi += 1
    loop do
        loop do
            lo += 1
            break if arr[lo] >= pivot
        end

        loop do
            hi -= 1
            break if arr[hi] <= pivot
        end
        return lo if lo >= hi
        arr[lo], arr[hi] = arr[hi], arr[lo]
    end
end 
```

随机和近似排序数据的算法运行时间

```
numbers = []
1000000.times do 
    numbers << r.rand(500000)
end 
```

随机数:

```
Bubble sort (1 million numbers): 33323.8596392 s, (555.4 mins)

Insertion sort (1 million numbers): 13213.5302226 s, (220.2 mins)

Merge sort (1 million numbers): 2.0483875 s

Quick sort (1 million numbers): 3.6877069 s

Ruby's native #sort (1 million numbers): 1.0330547 s, (a very optimized Quicksort algorithm) 
```

一个非常常见的用例是，必须对一个已经排序完毕的集合进行排序。让我们看看算法是如何处理的:

```
Ruby's native #sort (mostly sorted 1 million nums): 0.6229407 s (a very optimized Quicksort algorithm)

Merge sort (mostly sorted 1 million nums): 1.5681328 s 

Insertion sort (mostly sorted 1 million nums): 0.1712918 s 
```

如果你知道你需要频繁地对一个几乎已经排序的集合进行排序，并且你想要尽可能快的排序时间，一个简单的插入排序算法实际上胜过了完全优化的内置 Ruby #sort，在这种情况下是 3 倍。

*   快速排序:避免最坏的情况
    *   当我试图在大部分排序的列表上运行 QuickSort 时，程序崩溃了，直到我将数据集大小从 100 万一直降低到大约 15，000。

```
Quick sort (mostly sorted 15670 nums): 3.8133098 s 
```

*   在这一点上，这 15，000 个排序元素的运行时间与在 100 万个随机元素上运行的 QuickSort 相同，虽然 quicksort 递归的数量更少，但是在 partition 方法中出现的循环数量却明显更多。
*   如上所述，这是因为支点在数组的末尾，所以`arr`中的每一个其他元素都小于支点！我们不是制造两个偶数子问题来实现 nlgn 运行时间，而是创建两个大小为 1 和 n-1 的子问题。1 到 n 的和是θ(n^2)。想想 1 到 10 的求和。就是`(10 + 1) * (10 / 2) = 55`。用 n 代替 10，`(n + 1) * (n/2) = (n^2 )/2 + n/2`。去掉系数和低阶项得到`n^2`。

*   为了解决这个问题的θ(n^2)运行时的一个近似排序的列表，我们可以聪明地选择一个枢纽元素。如果我运行 optimize_pivot 来选择我的轴心点，我会在子数组中找到 low、hi 和 med 索引值之间的中间值。然后，我用`arr[hi]`切换该值，并继续使用`hi`作为我的枢纽索引。如果数组已经排序，我现在使用列表的中值而不是极值，并且可以在排序列表上实现θ(n * lgn)运行时，因为我回到了将问题分成两半，而不是从大小中减去一。

*   更好的快速排序代码

```
def quicksort_opt!(coll, lo = 0, hi = nil)
    hi = coll.length - 1 if hi.nil?
    if lo < hi
        partition_index = partition_opt(coll, lo, hi)
        quicksort_opt!(coll, lo, partition_index - 1)
        quicksort_opt!(coll, partition_index, hi)
    end
end

def partition_opt(arr, lo, hi)
     optimize_pivot(arr, lo, hi)
     pivot = arr[hi]
     lo -= 1
     hi += 1
     loop do
         loop do
             lo += 1
             break if arr[lo] >= pivot
         end

         loop do
             hi -= 1
             break if arr[hi] <= pivot
         end
         return lo if lo >= hi
         arr[lo], arr[hi] = arr[hi], arr[lo]
     end
end

def optimize_pivot(arr, lo, hi)
    mid = (lo + hi) / 2
    arr[lo], arr[mid] = arr[mid], arr[lo] if arr[mid] < arr[lo]
    arr[lo], arr[hi] = arr[hi], arr[lo] if arr[hi] < arr[lo]
    arr[mid], arr[hi] = arr[hi], arr[mid] if arr[mid] < arr[hi]
end 
```

这是所有算法的第二个性能指标，包括优化的快速排序

```
Ruby's native sort (1 million numbers): 1.1003078 s
Merge sort (1 million nums): 2.1523891 s
Quick sort (1 million nums): 3.8681517 s
Quick sort optimized (1 million nums): 3.8711199 s
Ruby sort (mostly sorted 1 million nums): 0.5628704 s
Merge sort (mostly sorted 1 million nums): 1.453362 s
Optimized quicksort (mostly sorted 1 million nums): 2.6978179 s
Insertion sort (mostly sorted 1 million nums): 0.1951487 s 
```

* * *

## 资源

[1] *算法简介*，科尔曼，莱瑟森，里维斯特，斯坦