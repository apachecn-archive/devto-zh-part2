# 理解神经网络:什么，如何和为什么？

> 原文：<https://dev.to/ugis22/understanding-neural-networks-what-how-and-why-4ab4>

[![](../Images/85c53df36a2e8f5c9e0fbb989eafc27a.png)T2】](https://cdn-images-1.medium.com/max/1024/1*J4WR2GliSeq5C2YdPKAkAQ.jpeg)

#### 解开*黑盒*

当谈到称为深度学习的机器学习子领域时，神经网络是最强大和最广泛使用的算法之一。乍一看，神经网络似乎是一个黑箱。一个输入层将数据输入到“*隐藏层*，变戏法之后我们可以看到由*输出层*提供的信息。然而，理解隐藏层在做什么是神经网络实现和优化的关键步骤。

在我们理解神经网络的道路上，我们将回答三个问题:*什么*、*如何*和*为什么*？

#### **什么是神经网络？**

我们将要考虑的神经网络被严格地称为人工神经网络，顾名思义，它是基于科学对人脑结构和功能的了解。

简而言之，神经网络被定义为一种计算系统，它由许多简单但高度互连的元素或节点组成，这些元素或节点被称为“神经元”，它们被组织成使用对外部输入的动态状态响应来处理信息的层。正如我们将在后面解释的那样，这种算法在寻找太复杂而无法手动提取和教会机器识别的模式方面非常有用。在这种结构的背景下，模式通过*输入* *层*引入到神经网络，该层对于输入数据中存在的每个分量具有一个神经元，并且被传送到网络中存在的一个或多个*隐藏层*；被称为“隐藏”仅仅是因为它们不构成输入或输出层。在隐藏层中，所有的处理实际上通过一个连接系统发生，该连接系统的特征在于 ***权重和偏差*** *(通常称为 W 和 b)* ***:*** 接收输入，神经元计算加权和，并根据结果和预设的**激活函数**(最常见的是 sigmoid、 *σ、*，即使它几乎不再使用然后，神经元将信息向下游传递给其他相连的神经元，这个过程被称为“前向传递”。在这个过程的最后，最后一个隐藏层被链接到*输出层*，它有一个神经元用于每个可能的期望输出。

[![](../Images/062ab06b1c636b023b317db84595f702.png)](https://cdn-images-1.medium.com/max/1024/1*pbk9xtz7WbBwYPVATdl9Vw.png) 

<figcaption>一个 2 层神经网络的基本结构。Wi:相应连接的重量。注意:计算网络中的层数时，不包括输入层。</figcaption>

#### 神经网络是如何工作的？

现在我们对神经网络的基本结构有了一个概念，我们将继续解释它是如何工作的。为了做到这一点，我们需要解释我们可以在网络中包含的不同类型的神经元。

我们要解释的第一类神经元是 ***感知器*** *。尽管今天它的使用已经衰退，但理解它们如何工作将为我们提供更多现代神经元如何运作的线索。*

感知器使用函数通过将二进制变量的向量映射到单个二进制输出来学习二进制分类器，并且它也可以用于监督学习。在这种情况下，感知器遵循以下步骤:

1.  将所有输入乘以它们的权重 ***w*** ，表示相应输入对输出有多重要的实数，
2.  将它们加在一起称为 ***加权和:∑ wj xj*** ，
3.  应用 ***激活函数*** ，换句话说，确定加权和是否大于一个*阈值*，其中-threshold 相当于 *bias，*并赋值 1 或更小，赋值 0 作为输出 _。_

我们也可以用下面的术语来写感知器函数:

[![](../Images/4ee6377c49c96722c75da50236b1f063.png)](https://cdn-images-1.medium.com/max/464/1*7y_U0_xQv5e5EUzDUJGvtw.png)T3】注:b 是偏差，相当于-阈值，w.x 是 w 的点积，w 是分量为权重的向量，x 是由输入组成的向量。

这种算法的最大优点之一是我们可以改变权重和偏差来获得不同的决策模型。我们可以给这些输入分配更多的权重，这样如果它们是积极的，它将有利于我们期望的输出。此外，因为偏差可以理解为输出 1 的难易程度的度量，所以如果我们想使期望的输出更可能或更不可能发生，我们可以降低或提高它的值。如果我们注意公式，我们可以观察到一个大的正偏差会使输出 1 变得非常容易；然而，非常负的偏差将使输出 1 的任务变得非常不可能。

因此，感知器可以分析不同的证据或数据，并根据设定的偏好做出决定。事实上，有可能创建更复杂的网络，包括更多层的感知器，其中每一层都采用前一层的输出并对其进行加权，从而做出越来越复杂的决策。

什么等一下:如果感知器可以很好地做出复杂的决定，为什么我们需要其他类型的神经元？包含感知器的网络的缺点之一是，即使只有一个感知器，权重或偏差的微小变化也会严重改变我们的输出，从 0 变为 1，反之亦然。我们真正想要的是能够通过引入权重或偏差的小修改来逐渐改变我们网络的行为。这就是一种更现代的神经元类型派上用场的地方(如今它的用途已被其他类型取代，如 Tanh 和最近的 ReLu): ***乙状结肠神经元。***sigmoid 神经元和感知器的主要区别在于，输入和输出可以是 0 到 1 之间的任何连续值。考虑到权重 w 和偏差 b，将 ***sigmoid 函数*** 应用于输入后，获得输出。为了更好地形象化，我们可以写下:

[![](../Images/bc40756720481326e599b660c702163a.png)T2】](https://cdn-images-1.medium.com/max/359/1*INRzr1dj-X3d0N-x7XRSpQ.png)

因此，输出的公式是:

[![](../Images/69fce6d4f14eaa6c7cd7e4742367846b.png)T2】](https://cdn-images-1.medium.com/max/237/1*OWFmaKDixkpbeAP-1hOe2Q.png)

如果我们对这个函数进行数学分析，我们可以做出我们的函数 _σ _ 的图形，如下所示，并得出结论:当 z 大且为正时，该函数达到其最大渐近值 1；然而，如果 z 很大并且是负的，则函数达到其最小渐近值 0。这就是 sigmoid 函数变得非常有趣的地方，因为在 z 值适中的情况下，函数呈现出平滑且接近线性的形状。在此区间内，权重(δwj)或偏置(δbj)的微小变化将导致输出的微小变化；我们所期望的行为是对感知机的改进。